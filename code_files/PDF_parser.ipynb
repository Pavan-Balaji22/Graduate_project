{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import nltk as nltk\n",
    "import os\n",
    "import re\n",
    "import fitz\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import torch\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_number(text):\n",
    "    text = re.sub(r'[^0-9]',\"\",text)\n",
    "    if len(text) > 0:\n",
    "        return int(text)\n",
    "    else:\n",
    "        return 0\n",
    "def load_cvs(cdir):\n",
    "    '''\n",
    "        Function to load all the cvs from a folder.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        cdir - String indicating the directory where Cvs are present\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        cv_s - list of document objects by PyMuPDF\n",
    "    '''\n",
    "    path = os.getcwd() + \"\\\\\" + cdir\n",
    "    CV_folders = os.listdir(path)\n",
    "    cv_s = {}\n",
    "    for i in CV_folders:\n",
    "        folder_path = path + \"\\\\\" + i\n",
    "        files = os.listdir(folder_path)\n",
    "        for j in files:\n",
    "            if re.search(\".pdf\",j) is not None:\n",
    "                name = re.sub(\".pdf\",\"\",j)\n",
    "                file = fitz.open(folder_path+\"\\\\\"+j)\n",
    "                cv = doc_to_txt(file)\n",
    "                cv_s[name]={\"CV\":cv,\"Department\": i,\"n_pages\":file.page_count}\n",
    "                file.close()\n",
    "            \n",
    "    return cv_s\n",
    "\n",
    "def process_text(up_text):\n",
    "    '''\n",
    "        Function to process the extracted text.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        up_text - unprocessed text\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        processed_text -  processed text\n",
    "    '''\n",
    "    processed_text = re.sub(r'\\n (?!=\\w)',\"\",up_text)\n",
    "    processed_text = re.sub(r'\\n\\d{1} (?!=\\d)',\"\",processed_text)\n",
    "    processed_text = re.sub(r'\\n\\d{1} (?!=\\d)',\"\",processed_text)\n",
    "#     processed_text = re.sub(r'\\d{1} (?!=\\w)',\"\",processed_text)\n",
    "    processed_text = re.sub(r'[_]',\"\",processed_text)\n",
    "    processed_text = re.sub(r'[â€¢]',\"\",processed_text)\n",
    "    return processed_text\n",
    "    \n",
    "\n",
    "def extract_text(document):\n",
    "    '''\n",
    "        Function to extract and process text from documents pages.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        document - A documents object containing CV\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        txt - text extracted from the documents object\n",
    "    '''\n",
    "    n_pages = document.page_count\n",
    "    txt = \"\"\n",
    "    for i in range(n_pages):\n",
    "        txt = txt+\" \"+document[i].get_text(\"text\",flags = 1)\n",
    "        txt = process_text(txt)\n",
    "    return txt\n",
    "    \n",
    "\n",
    "def doc_to_txt(CV_objects):\n",
    "    '''\n",
    "        Function to convert the PyMuPDF document objects to processed plain text data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        CV_objects - Document objects containing CVs\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        CV_objects- returns the object but each CV as a string  \n",
    "    '''\n",
    "    \n",
    "    CV_objects = extract_text(CV_objects)\n",
    "    return CV_objects\n",
    "\n",
    "def extract_req_data(original_data):\n",
    "    req_data = original_data.copy()\n",
    "    awards = list()\n",
    "    n_events = list()\n",
    "    nlp = spacy.load(\"en_core_web_trf\")\n",
    "    for i in req_data[\"CV\"]:\n",
    "        doc = nlp(i)\n",
    "        entities = list()\n",
    "        labels = list()\n",
    "        for ent in doc.ents:\n",
    "            entities.append(ent.text)\n",
    "            labels.append(ent.label_)\n",
    "        temp = pd.DataFrame(np.c_[entities,labels],columns=[\"Entity\",\"Label\"])\n",
    "        awards.append(temp[temp[\"Label\"] == \"MONEY\"][\"Entity\"].apply(text_to_number).sum())\n",
    "        if \"EVENT\" in temp[\"Label\"].unique():\n",
    "            n_events.append(temp[\"Label\"].value_counts()[\"EVENT\"])\n",
    "        else:\n",
    "             n_events.append(0)\n",
    "        del doc \n",
    "        torch.cuda.empty_cache() \n",
    "    \n",
    "    req_data[\"Awards\"] = awards\n",
    "    req_data[\"No of Event and Conferences\"]= n_events\n",
    "    return req_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_objs =  load_cvs(\"Public_CVs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(cv_objs,index=None).T\n",
    "data.reset_index(inplace=True)\n",
    "data.columns = [\"Name\",\"CV\",\"Department\",\"PageCount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_json(\"data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (661 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "req_data = extract_req_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_data = req_data.drop(columns=[\"CV\",\"PageCount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_data.to_csv(\"Resume_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
